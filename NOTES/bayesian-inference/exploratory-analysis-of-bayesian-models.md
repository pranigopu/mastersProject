**EXPLORATORY ANALYSIS OF BAYESIAN MODELS**

---

**Contents**:

- [Introduction](#introduction)
- [Understanding model assumptions](#understanding-model-assumptions)
- [Understanding model predictions](#understanding-model-predictions)

---

> **Main resource**: [_2.Exploratory Analysis of Bayesian Models_ from **Bayesian Computation Book**](https://bayesiancomputationbook.com/markdown/chp_02.html)

---

# Introduction
Bayesian modelling is wider than inference; apart from specifying the model and calculating a posterior, there exist other tasks that are necessary for successful Bayesian data analysis. In particular successful Bayesian modelling approach may require:

- Diagnosing the quality of the inference results obtained using numerical methods
- Model criticism, including evaluations of both model assumptions and model predictions
- Comparison of models, including model selection or model averaging
- Preparation of the results for a particular audience

# Understanding model assumptions
Model assumptions are quantified using the prior. However, a problem in choosing priors is understanding their effect on how the model is updated using the data. For example, the choices made in the parameter space (e.g. choices about what parameters to define and/or what constraints to define for their possible values) may lead to something unexpected in the predicted data space (e.g. predicting absurd values with a higher-than-reasonable plausibility), which in turn may lead to the model updating less-than-optimally.

If we are not sure how well the prior encodes information relevant to the domain, we have a few alternatives: (1) We can rethink our model to incorporate further relevant information and insights. (2) We can use a prior that reduces (though does not necessarily eliminate) the chance of nonsensical results. (3) We can just fit the data and see if the data itself is informative enough to estimate a posterior that excludes nonsensical values. But how do we know what to do? Evidently, we must in some way evaluate our prior, i.e. our model's assumptions.

Since prior information and/or assumptions are only meaningful with respect to the domain, we need to put them in the context of a domain. Furthermore, thinking in terms of synthetic data generated by the model is generally easier than thinking in terms of the model's parameters. We can evaluate the parameter values by evaluating the synthetic data generated by the corresponding models; such evaluation can be done using prior predictive distributions (PrPDs). PrPDs become even more useful for complex models where parameters are transformed in many ways or steps, or where many priors (i.e. prior information and/or assumptions) interact with each other.

**NOTE**: _The synthetic data (generated by the model) is data that reveals what the model expects for future outcomes. If the model is sufficiently accurate, then the synthetic data reveals the kind of data we may expect to actually observe._

Additionally, PrPDs can be used to present results or discuss models in a more intuitive way to a wide audience. For example, a domain expert may not be familiar with statistical notation or code and thus using those devices to communicate the model may not be productive, but if you show them the implications of one or more models in terms of synthetic data, there is more accessible and interpretable material to discuss. which can provide valuable insights both for the domain expert and yourself. Furthermore, computing the PrPD has other advantages, such as helping us debug models and ensuring that they are properly written and able to run in our computational environment.

# Understanding model predictions
Just as we can use synthetic data, i.e. potentially observed data, or more precisely, generated data, from the PrPD to help us inspect our model's plausibility and reasonability, we can perform a similar analysis with the posterior predictive distribution, introduced in Section Bayesian Inference and Equation (1.8). This procedure is generally referred as posterior predictive checks. The basic idea is to evaluate how close the synthetic observations are to the actual observations. Ideally the way we assess closeness should be problem-dependent, but we can also use some general rules. We may even want to use more than one metric in order to assess different ways our models (mis)match the data.
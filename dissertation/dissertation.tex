\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage{float}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{tabularx}
\usepackage{hyperref}
\usepackage{float}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}
\bibliographystyle{unsrt}

\title{Comparative Study of BNN Implementations\\}

\author{\IEEEauthorblockN{Pranav Narendra Gopalkrishna\\231052045\\Paulo Rauber\\MSc. Artificial Intelligence}}

\maketitle

\begin{abstract}
Deep learning using neural networks (NNs) lacks transparency about the model's confidence in its predictions. Bayesian neural networks (BNNs) are a means to achieve a level of transparency and interpretability in deep learning by using Bayesian inference (BI) to quantify the model's uncertainty about its predictions. The link between BI and BNNs is not easy, and implementing BNNs in practice needs significant mathematical and algorithmic depth. Furthermore, there exist both exact and approximate approaches to BNNs, the former offering accuracy, the latter offering efficiency. In this study, the two approaches are explored in theoretical and practical detail, using (for reasons discussed later) Hamiltonian Monte Carlo (HMC) as the exact BI method, and Variational Inference (VI) as the approximate BI method. The implementations of each are applied to four synthetic regression problems and are evaluated by observing (1) the average accuracy of their predictions and (2) the quality of the quantification of their uncertainty about their predictions.
\end{abstract}

\begin{IEEEkeywords}
Bayesian Inference (BI), Markov Chain Monte Carlo (MCMC), Variational Inference (VI), Hamiltonian Monte Carlo (HMC), Neural Network (NN), Bayesian Neural Network (BNN)
\end{IEEEkeywords}

\section{Introduction}
A BNN is a kind of NN that can quantify its confidence about its predictions, or more precisely, its uncertainty about its predictions. Furthermore, using the framework of BI, it can explain – mathematically – how this uncertainty is quantified. To elaborate, a BNN is a stochastic neural network (SNN) trained using BI (Jospin et al., 2020). An SNN is an NN with stochastic elements (e.g. stochastic weights or stochastic activations) that, through its stochasticity, simulates a number of models according to some probability distribution of models; thus, SNNs are a special case of ensemble learning. Thus, a BNN is an SNN whose models are distributed by a posterior for the parameterisations of a more generalised model.\\

There are practical reasons to explore BNNs. Firstly, BNNs help distinguish between epistemic uncertainty (uncertainty due to lack of knowledge) and aleatoric uncertainty (uncertainty due to the model's inherent stochasticity, i.e. uncertainty due to factors that cannot be accounted for practically); thus, BNNs help avoid overfitting, since when making predictions, the model gives a high epistemic uncertainty for points outside the expected distribution of data points (which is based on the training data points) instead of a prediction that is likely to be incorrect (Jospin et al., 2020). Secondly, every deep learning model has assumptions about the distribution of its parameters; these assumptions are implicit in the initialisation of its weights and measures (if any) to avoid overfitting, such as regularisation and drop-out. A BNN makes assumptions about its parameters explicit through its prior, thus making the model more transparent and interpretable.\\


Thus, BNNs are a means to achieve transparency, interpretability and generalisability in deep learning. In light of the practical value of BNNs as well as their theoretical and practical complexity, the goals of this study are as follows: (1) bridging the gap between BNN’s theory and its implementation, (2) implementing two fundamentally distinct approaches to quantifying the uncertainty of an NN's predictions, (3) evaluating the results of the aforementioned approaches, and (4) making the theory and practice of BNNs accessible and reproducible to someone new to BI and BNNs.\\

As indicated, the focus shall be on two fundamentally distinct classes of BI methods: (1) BNN via an exact BI method, and (2) a BNN via an approximate BI method. Due to being theoretically well-founded and practically well-established, HMC (a class of MCMC methods) and VI are used for an exact and approximate BI method respectively; their theoretical and practical details shall be discussed in later sections.\\

\section{Related Work}
Jospin et al. (2020) give an in-depth introduction to BI for deep learning, covering a wide range of topics from BI algorithms to performance metric for BNNs. BI itself (from the mathematics to the algorithms) is covered more deeply by Martin et al. (2021) in their textbook "Bayesian Modelling and Computation in Python"; notably, the textbook also introduces MCMC methods (Metropolis-Hastings and HMC) and VI. On the other hand, Chandra and Simmons (2023) give a practical tutorial for implementing MCMC using Python, starting from MCMC for a single-valued parameter and progressing to linear regression via MCMC and finally to BNNs via MCMC.\\

Using four evaluation metrics (validity of the confidence intervals, distance to the HMC reference, distance to the target posterior and similarities between the algorithms), Brian and Da Veiga (2022) evaluate the performance of a wide range of approximation methods for BNNs on synthetic regression tasks. In practice, where parameter spaces tend to be high-dimensional (due to complex models, e.g. deep learning models) and datasets tend to be large (due to complex problems, e.g. image analysis), approximate BI methods are key areas of study. Yao et al. (2019) focus on evaluating the quality of uncertainty quantification using empirical comparison of 8 state-of-the-art approximate BI methods and 2 non-Bayesian frameworks. On the other hand, Foong et. al (2019) focus not only on evaluating approximate BI methods but also exploring pathologies arising due to approximation.

\section{Relevant Concepts}
\subsection{Bayesian Inference (BI)}
BI is the process of inferring a generative model\footnote{A model that generates data based on a well-defined random process.} that explains the observed data. BI consists of (1) proposing a generative model parameterised by $\theta$, (2) making judgements about the model's parameterisations prior to considering the observed data $D$, (3) considering $D$ by measuring the likelihood of the model generating $D$ for the given parameterisation, and (4) measuring the plausibility — or more precisely, the posterior probability — of the model's parameterisations, given the prior judgements and $D$. Hence, note that the generative model and prior judgements about it are set before the inference (e.g. using assumptions and/or knowledge apart from $D$) and the likelihood is based on the generative model. Hence, BI seeks the posterior probability of the model's parameterisations based on $D$. Mathematically, BI is based on Bayes' theorem:

\begin{equation*}
    P(\theta|D) = \frac{P(D|\theta)P(\theta)}{P(D)}
\end{equation*}

\begin{tabular}{| m{1.5cm} | m{6cm} |}
    \hline
    $\theta$ & A parameterisation of the model\\
    \hline
    $D$ & The observed data\\
    \hline
    $P$ & The probability measure\\
    \hline
    $P(\theta|D)$ & The posterior\\
    \hline
    $P(D|\theta)$ & The likelihood\\
    \hline
    $P(\theta)$ & The prior\\
    \hline
    $P(D)$ & The evidence\\
    \hline
\end{tabular}\\~\\

The evidence term $P(D)$ is used to normalise the numerator $P(D|\theta)P(\theta)$; without this, we only have $P(D|\theta)P(\theta) = P(\theta, D)$, i.e. the joint probability of $\theta$ and $D$ rather than the conditional probability of $\theta$ given $D$. The evidence term can be interpreted as the total probability of the observed data being generated by the model for any hypothetical parameterisation. Mathematically, if $\Theta$ is the set of all hypothetical parameterisations, then $P(D) = \int_{\theta \in \Theta} P(D|\theta)P(\theta) d\theta$. In practice, however, the $P(D)$ is often intractable and thus BI methods (e.g. MCMC and VI) have been developed to estimate the posterior through just the unnormalised posterior, i.e. $P(D|\theta)P(\theta)$.

\subsection{Exact vs. Approximate BI}
A exact BI method (e.g. MCMC) is one that is theoretically guaranteed to converge to posterior. An approximate BI method (e.g. VI) is one that uses some approximation of the posterior that is not theoretically guaranteed to converge to the posterior; the goal of such a method is to minimise the distance between the approximation and the posterior.

\subsection{Markov Chain Monte Carlo (MCMC)}
It is key to first understand MCMC, since HMC is a class of MCMC methods. MCMC is a class of methods for sampling from a target distribution. Hence, it is an exact BI method when the target distribution is the posterior $P(\theta|D)$, which is the probability distribution of the model parameter $\theta$ as conditioned by the observed data $D$.\\

MCMC works when we know the prior $P(\theta)$ and likelihood $P(D|\theta)$ and thereby know the unnormalised posterior $P(D|\theta)P(\theta) \propto P(\theta|D)$. Using the unnormalised posterior, we can calculate the probability $\alpha$ of a proposed sample $\theta'$ of $\theta$ being drawn from the posterior. In MCMC, $\alpha$ is used as the acceptance probability of $\theta'$, i.e. the probability of accepting $\theta'$ to be a sample from the posterior. To make proposals for new samples, MCMC sets only one condition: the probability of proposing a new sample must depend on the current sample. The motivation for this condition is that when a sample has a high probability of being from the posterior, there must be a method, using this sample as the starting point, to find other samples with a high probability of being from the posterior.\\

For example, Metropolis-Hastings (an MCMC method) proposes new samples using a random walk starting from the current sample, under the assumption that when the current sample has a high probability of being from the posterior, so do the samples in its vicinity\footnote{Seeking higher probability samples does not preclude the inclusion of lower probability samples; indeed, for an accurate estimation of the target distribution, we need all kinds of samples. But the proportion of each kind of sample drawn corresponds to its probability with respect to the target distribution.}. In effect, MCMC results in a \href{https://github.com/pranigopu/mastersProject/tree/main/conceptual-notes/markov-chains}{Markov chain} of Monte Carlo samples (hence its name), with each state of the Markov chain being a sample from the posterior. More precisely, the transition probabilities of MCMC's Markov chain, which are based on the acceptance probability $\alpha$, are such that (1) the Markov chain is theoretically guaranteed to converge to a steady-state (i.e. a state where long-range transition probabilities are stable), and (2) the steady-state probabilities (i.e. long-range transition probabilities) are theoretically guaranteed to correspond to the probabilities of sampling from the target distribution. To summarise, MCMC goes through the following steps:\\

\begin{enumerate}
    \item Set the current sample with an initial value $\theta_0$
    \item Using a proposal method, propose a new sample $\theta'$
    \item Calculate the acceptance probability $\alpha$ of $\theta'$
    \item If $\theta'$ is accepted, set $\theta'$ as the current sample
    \item Repeat from step 2 for a fixed number of iterations\\
\end{enumerate}

\subsection{Hamiltonian Monte Carlo (HMC)}
HMC is a class of MCMC methods that uses gradients of the negative log-probability of the posterior to generate new proposed states, i.e. new samples proposed to be from the target distribution (the reason for using the negative log-probability of the posterior is explained in appendix A). The gradients of the negative log-probability of the posterior evaluated at a given state (i.e. a given sample) give information about the posterior density function's geometry. Owing to the geometric interpretation of HMC, states (i.e. samples) are also called positions.\\

As with MCMC, only the unnormalised posterior $P(D|\theta)P(\theta)$ is needed for HMC, because the gradients of the negative log-probability are equal for the posterior $P(\theta|D)$ and the unnormalised posterior $P(D|\theta)P(\theta)$, as show below:\\

$\displaystyle - \frac{\delta}{\delta \theta} \log P(\theta|D) = - \frac{\delta}{\delta \theta} \log \frac{P(D|\theta)P(\theta)}{P(D)}$ (by Bayes' rule)

$\displaystyle = - \frac{\delta}{\delta \theta} (\log P(D|\theta) P(\theta) - P(D))$

$\displaystyle = - \frac{\delta}{\delta \theta} \log P(D|\theta) P(\theta) + \frac{\delta}{\delta \theta} P(D)$

$\displaystyle = - \frac{\delta}{\delta \theta} \log P(D|\theta) P(\theta)$ (since $P(D)$ is a constant)\\

HMC tries to avoid the random walk behaviour typical of Metropolis-Hastings by using gradients to propose new positions (i.e. new samples) that is both far from the current position (i.e. current sample) and with high acceptance probability (Martin et al., 2021). This allows HMC to scale well to higher dimensions and, in principle, to more complex geometries compared to other MCMC methods (Martin et al., 2021). Intuitively, we can think of HMC as a Metropolis-Hasting algorithm with a superior sample proposal distribution (Martin et al., 2021). The HMC algorithm is given below (Carroll, 2019):\\

\begin{algorithm}{H}
\textbf{input} $n, -\log p, \theta_0, T, \Delta t$
\caption{Run HMC Sampler}
\begin{algorithmic}[H]
\State $\text{samples} \gets \text{array of size } n$
\State $g \gets \text{grad}(-\log p)$
\For{$i \in (0, 1, 2 ... n-1)$}                
    \State $m_0 \gets \text{draw}(\mathcal{N}(0, \sigma I))$
    \State $\theta_T, m_T \gets \text{leapfrog}(\theta_0, m, g, T, \Delta t)$
    \State $H_\text{old} \gets -\log(p)(\theta_0) - \log \mathcal{N}(0, \sigma I)(m_0)$
    \State $H_\text{new} \gets -\log(p)(\theta_T) - \log \mathcal{N}(0, \sigma I)(m_T)$
    \State $\alpha \gets e^{H_\text{old} - H_\text{new}}$
    
    \If{$\text{draw}(\mathcal{U}(0, 1)) < \alpha$}
        \State $\text{samples}[i] \gets \theta_T$
    \Else
        \State $\text{samples}[i] \gets \theta_0$
    \EndIf
\EndFor
\end{algorithmic}
\textbf{return} $\text{samples}$
\end{algorithm}

\begin{tabular}{| m{1.5cm} | m{6cm} |}
    \hline
    $n$ & Number of samples to draw\\
    \hline
    $p$ & Target probability distribution\\
    \hline
    $-\log p$ & Negative log-probability of $p$\\
    \hline
    $T, \Delta t$ & Number and size of leapfrog steps\\
    \hline
    $\theta_0, m_0$ & Current position and momentum\\
    \hline
    $\theta_T, m_T$ & Proposed position and momentum\\
    \hline
    $\alpha$ & Acceptance probability of proposal\\
    \hline
    $H_\text{old}$ & Hamiltonian function for $\theta_0, m_0$\\
    \hline
    $H_\text{new}$ & Hamiltonian function for $\theta_T, m_T$\\
    \hline
    $\mathcal{N}(\mu, \sigma)$ & Normal distribution\\
    \hline
    $\mathcal{U}([a, b])$ & Uniform distribution\\
    \hline
    $\text{grad}$ & Returns callable gradient\\
    \hline
    $\text{draw}$ & Function to draw from given distribution\\
    \hline
    $\text{leapfrog}$ & Leapfrog integrator\\
    \hline
\end{tabular}\\~\\

HMC can be explained using a physical analogy, wherein $-\log p$ defines the contours of a force field (e.g. a gravitational field) along which a body — the sampler, in our case — can travel, with each position representing a sample. Starting from the current position $\theta_0$, the leapfrog integrator is a symplectic integrator that uses the gradient of $-\log p$ and the current momentum $m_0$ to simulate the exact trajectory of the sampler along the contours of $-\log p$ for $L = \frac{T}{\Delta t}$ discrete time steps. Thus, a symplectic integrator discretises the Hamiltonian equations used to explore the posterior. If the momentum is well-chosen, the trajectory travels through positions with the same or similar acceptance probabilities as the current position. However, despite its accuracy, a symplectic integrator is likely to introduce at least some errors in the calculation of trajectories (Betancourt, 2018). Furthermore, the momentum may be sub-optimal. To account for such errors, a proposed sample is accepted based on the Metropolis criterion, i.e. a proposed sample is accepted with the acceptance probability $\alpha = \min(1, e^{H_\text{old} - H_\text{new}})$ (Neal, 2011). The mathematical details and reasoning are given in appendix B.

\subsection{Variational Inference (VI)}
VI is a method of analytically approximating the target distribution. More precisely, VI approximates the target distribution $p$ using a distribution $q_\phi$ — called the variational distribution — parameterised by $\phi$. To approximate $p$ using $q_{\phi}$, $\phi$ is optimised to minimise the distance between $p$ and $q_{\phi}$; a well-established measure of distance is the Kullback-Leibler divergence (KL-divergence). Hence, when applied to BI, VI is an approximate BI method, the target distribution being the posterior.\\

In theory, MCMC converges to the posterior, but in practice, this convergence may be inefficient or even infeasible due to the posterior's complexity arising from the model's complexity. After all, converging to the posterior through samples is a statistical inference problem that has a need for sufficient sample quantity and quality; the higher the posterior's complexity, the higher the need. VI, on the other hand, simplifies the statistical inference problem to an optimisation problem (Ganguly and Earp, , 2021), gaining efficiency while losing the theoretical guarantee of convergence, since in general, there is no theoretical guarantee that a known distribution can converge to an unknown distribution.\\

A well-established method to perform VI is with the use of the Evidence Lower Bound (ELBO), which is derived from the KL-divergence between the variational distribution and the posterior (see appendix C). However, the focus shall be on the method used in \textit{torchbnn}, a PyTorch-based implementation of BNNs (Kim, 2020; Lee et al., 2021), since this method can be more easily applied to a neural network. This implementation is an adversarial BNN based on the paper by Lee et al. (2021), which, while not a VI method explicitly, shall now be shown to be a VI method implicitly, i.e. in essence.\\

Given an ANN, the variational distribution is taken as a distribution whose parameters are functions of the existing weights and/or biases of the ANN. For example, the variational distribution can be a normal distribution parameterised by $(\mu_q, \sigma_q)$, where $\mu_q$ and $\sigma_q$ are the mean and standard deviation of the ANN's weights. Given a well-defined prior, the training is done using (1) a point estimation loss chosen as per the given problem (e.g. mean squared error, for regression problems), and (2) the KL-divergence between the variational distribution and the prior. Hence, the sum of the point estimation loss and KL-divergence serves as a measure of distance between the variational distribution and the posterior.

\section{Methodology}
The methodological focus is on four key areas: (1) well-motivated synthetic regression problems, (2) a BNN via an exact BI method, (3) a BNN via an approximate BI method, and (4) methods of evaluating the performance of the aforementioned BNNs. The implementations are in Python, using both TensorFlow and PyTorch. Note that for both kinds of BNNs, the training parameters (namely the learning rate, number of epochs and batch size) were chosen by trail-and-error based on which values produced a similarly high levels of average accuracy (i.e. after having averaged the samples/predictions).

\subsection{Synthetic Regression Problems}
The focus is not on the predictive accuracy of the models but rather the models' ability to quantify the uncertainty about the process (real-life or synthetic) that is generating the observed data. Hence, the problems need the be such that:\\

\begin{itemize}
    \item Basic NN models can accurately train for them
    \item The data is noisy enough to cause uncertainty
    \item The data has complexities leading to areas of uncertainty\\
\end{itemize}

The four synthetic regression problems used by Brian and Da Veiga (2022) are such that each problem meets some or all of the above requirements; note that the problems that meet only specific requirements help focus on specific aspects of the BNNs' performance. The problems have been changed without changing their essence and are as follows (note that $\mathcal{N}(\mu, \sigma)$ denotes a normal distribution with mean $\mu$ and standard deviation $\sigma$, whereas $\mathcal{U}([a, b])$ denotes a uniform distribution over the interval $[a, b]$):\\

\textbf{Synthetic Problem A}

\begin{tabular}{m{2.5cm} | m{6cm}}
    Outputs & $y_i = \cos{2x_i} + \sin{x_i} + \epsilon_i$\\
    Error term & $\epsilon_i \sim \mathcal{N}(0, 0.25)$\\
    Train inputs & $x_i \sim \mathcal{U}([-3, 3])$\\
    Test inputs & $x_i \sim \mathcal{U}([-3, 3])$
\end{tabular}\\

\textbf{Synthetic Problem B}

\begin{tabular}{m{2.5cm} | m{6cm}}
    Outputs & $y_i = 0.1x_i^3 - x + \epsilon_i$\\
    Error term & $\epsilon_i \sim \mathcal{N}(0, 0.25)$\\
    Train inputs & $x_i \sim \mathcal{U}([-4, 1] \bigcup [3, 4])$\\
    Test inputs & $x_i \sim \mathcal{U}([-4, 4])$
\end{tabular}\\

\textbf{Synthetic Problem C}

\begin{tabular}{m{2.5cm} | m{6cm}}
    Outputs & $y_i = -(1 + x_i)\sin(1.2x_i) + \epsilon_i$\\
    Error term & $\epsilon_i \sim \mathcal{N}(0, 0.5)$\\
    95\% train inputs & $x_i \sim \mathcal{U}([-6, 2] \bigcup [2, 6])$\\
    5\% train inputs & $x_i \sim \mathcal{U}([-2, 2])$\\
    Test inputs & $x_i \sim \mathcal{U}([-6, 6])$
\end{tabular}\\

\textbf{Synthetic Problem D}

\begin{tabular}{m{2.5cm} | m{6cm}}
    Outputs & $y_i = f(x_i, w) + \epsilon_i$\\
    Weights & $w \sim \mathcal{N}(0, I_d)$\\
    Error term & $\epsilon_i \sim \mathcal{N}(0, 500)$\\
    Training inputs & $x_i \sim \mathcal{U}([-10, 6] \bigcup [6, 10] \bigcup [14, 18])$\\
    Test inputs & $x_i \sim \mathcal{U}([-12, 22])$
\end{tabular}\\~\\

Note that $f$ in synthetic problem D denotes a feed-forward neural network with three hidden layers with 100 neurons each (thus, 20501 parameters in total); the network's weights $w$ are sampled once from a standard multivariate Gaussian distribution $\mathcal{N}(0, I_d)$.\\

Problems A and C introduce complexity in their functional forms, problems B and D introduce gaps in the data, and problem C introduces data sparsity within the interval $[-2, 2]$. In each problem, the noise terms $\epsilon_i$ are normally distributed so that the outputs are normally distributed, hence making normal priors and likelihoods viable in the later BIs; such an approach is motivated by the fact that normal distributions are easy to work with mathematically. The standard deviations of the noise terms were chosen by trial and error. Hence, the problems introduce uncertainty through noise, shape, gaps and sparsity.\\

\subsection{HMC Implementation}
\subsubsection{Functional Model}
An artificial neural network (ANN), implemented using TensorFlow's Keras API, is used as the generative model, i.e. functional model for BI. The ANN's architecture is meant to simple enough to allow for as efficient sampling as possible, yet complex enough to allow for the accurate modelling of each synthetic regression problem. The ANN's architecture is as follows (note that the parameters include bias terms, 100 per layer):\\

\begin{tabular}{| m{1cm} | m{1.5cm} | m{2cm} |}
    \hline
    \textbf{Layer} & \textbf{Shape} & \textbf{Parameters}\\
    \hline
    Input & (1, 100) & 200\\
    \hline
    Hidden & (100, 100) & 10100\\
    \hline
    Output & (100, 1) & 101\\
    \hline
\end{tabular}\\~\\

The ANN is trained for $500$ epochs and a batch size of $32$, using a learning rate of $0.01$. The ANN's weights are initialised by random values and are trained using stochastic gradient descent with mean squared error loss. To improve the ANN's performance, (1) per training loop, only the weights of the epoch with the lowest loss are saved, and (2) $5$ training loops are run for the same regression problem, with the weights reinitialised at the start of every loop. In the end, the best performing weights (i.e. the weights with the lowest loss) are chosen.

\par\noindent\rule{0.49\textwidth}{0.1pt}

\subsubsection{HMC Sampler}
The HMC sampler used is the \textit{HamiltonianMonteCarlo} kernel from the TensorFlow Probability MCMC Package. The number of samples to be drawn is set as $20000$, since this was sufficient to lead to a more than $50\%$ acceptance rate. The number of burn-in steps (i.e. the steps for which the Markov chain is considered to be converging to its steady-state) is set as $2500$.\\

To discretise the Hamiltonian equations used to explore the posterior, this HMC sampler uses the leapfrog integrator, which has two hyperparameters: (1) step-size $\Delta t$ (i.e. the size of the discrete time steps\footnote{"Time" here is fictitious, referring to the physical analogy.} for which the Hamiltonian equations are solved, thereby simulating the sampler's trajectory when exploring the posterior) and (2) the number of leapfrog steps $L$; hence, the length of the sampler's simulated trajectory from the current sample to the proposed sample is $T = L \cdot \Delta t$.\\

Based on Neal (2012, p. 135), if the step-size is too large, the discretisation errors of the simulated trajectories get so large that the proposed samples have a low acceptance rate, i.e. there are to too many sub-optimal proposals. If the step-size is too small, the simulated trajectory's length $T$ gets so small that the exploration of the posterior is too inefficient, possibly more inefficient than even a random walk. To avoid such issues, the simple step-size adaptation policy is used, wherein the step-size is not fixed before sampling but rather multiplicatively increased or decreased during sampling (tfp.mcmc.SimpleStepSizeAdaptation), based on the logarithm of the acceptance probabilities; this policy is based on equation 19 of Andrieu and Thoms (2008). To achieve this policy in code, the \textit{HamiltonianMonteCarlo} kernel is wrapped in the \textit{SimpleStepSizeAdaptation} kernel from the TensorFlow Probability MCMC Package. The number of adaptation steps is given as 0.8 times the number of burn-in steps.\\

The simulated trajectory's length $T = L \cdot \Delta t$ is key to the HMC exploring the state space systematically rather than by a random walk (Neal, 2011, p. 137). Having chosen the step-size $\Delta t$, the number of leapfrog steps $L$ must now be chosen. Choosing the number of leapfrog steps $L$ for a complex problem has to be done through trial-and-error; in the case of exploring the posterior of the weights of a neural network, which is a high-dimensional, non-convex and thus multi-modal distribution, a high number of leapfrog steps such as $L = 100$ may be suitable (Neal, 2011, p. 137) and is hence chosen.

\par\noindent\rule{0.49\textwidth}{0.1pt}

\subsubsection{BI Components}
To perform HMC sampling, the likelihood $P(D|\theta)$ and prior $P(\theta)$ must be defined so as to define the unnormalised posterior $P(D|\theta)P(\theta) \propto P(\theta|D)$ (as seen before, the HMC sampler only needs the gradients of the unnormalised posterior with respect to the position $\theta$).\\

For basic architectures as used in Bayesian regression, it is standard to use a multivariate normal prior with a zero mean and a diagonal covariance $\sigma I_d$ on the coefficients of the ANN. In other words, the prior assumption is that $\theta \sim \mathcal{N}(0, \sigma I_d)$. Hence, the prior density of a parameterisation $\theta$ is given by:

\begin{equation*}
	P(\theta) = \mathcal{N}(0, \sigma I_d)(\theta)
\end{equation*}

Here, $I_d$ is an identity matrix of $d$ dimensions, where $d$ is the dimensionality of $\theta$. There is no theoretical argument that makes such a prior superior to any other (Jospin et al., 2020), but in practice, if lacking more well-motivated priors, a zero mean normal prior is preferred due to (1) its mathematical properties and (2) the ease of taking its logarithm (logarithms are used in machine learning to prevent arithmetic overflow). Furthermore, such a prior serves to make the ANN to keep its weights within a range unless there is overwhelming evidence to the contrary. In particular, a multivariate normal prior with mean $0$ and diagonal covariance $\sigma I_d$ is equivalent to a weighted $l_2$ regularisation when training a point estimate ANN (Jospin et al., 2020).\\

A regression problem models the output (i.e. the target) $y_i$ as $y_i = f_\theta(x_i) + \epsilon_i$, where $x_i$ is the input, $f_\theta$ is the functional model parameterised by $\theta$, and $epsilon_i$ is the error term. Here, the error term is taken to be distributed by $\mathcal{N}(0, \tau)$. The likelihood of the observed data $D$ given the parameterisation $\theta$ presupposes (1) the observed data $D = (x, y)$, where $x = (x_1, x_2 ... x_n)$ and $y = (y_1, y_2... y_n)$ (here, we consider the inputs as one-dimensional) (2) the functional model $f_\theta$ as parameterised by $\theta$, and (3) the distribution of error terms. Note that when taking the likelihood, $D$ and $\theta$ are taken as constant. Hence, the target $y$ is also distributed by a normal distribution, its standard deviation being the same as the standard deviation for the error term. More precisely, given $\hat{y_i} = f_\theta(x_i)$, we have that $y_i \sim \mathcal{N}(f_\theta(x_i), \tau)$ (note that $\tau$ is assumed to be known, and is assigned the exact value of the standard deviation of the given synthetic regression problem's error term $\epsilon_i$). Hence, the likelihood density for $y = (y_1, y_2... y_n)$ is:

\begin{equation*}
	\mathcal{N}(f_\theta(x), \tau)(y) = \prod_{i=1}^n \mathcal{N}(f_\theta(x_i), \tau)(y_i)
\end{equation*}

The above product would lead to arithmetic overflow if there are a substantial number of data points, i.e. if $n$ is to large, and thus, in practice, the log-likelihood density is used:

\begin{equation*}
	\log \prod_{i=1}^n \mathcal{N}(f_\theta(x_i), \tau)(y_i) = \sum_{i=1}^n \log \mathcal{N}(f_\theta(x_i), \tau)(y_i)
\end{equation*}

The \textit{HamiltonianMonteCarlo} kernel from the TensorFlow Probability MCMC Package takes in a callable for the target log-probability, which becomes the log-probability of the unnormalised posterior for BI. Hence, this callable is to return the following:\\

$\log P(D|\theta)P(\theta) = \log P(D|\theta) + \log P(\theta)$\\

$= \log \mathcal{N}(f_\theta(x), \tau)(y) + \log \mathcal{N}(0, \sigma I_d)(\theta)$\\

$\displaystyle = \sum_{i=1}^n \log \mathcal{N}(f_\theta(x_i), \tau)(y_i) + \log \mathcal{N}(0, \sigma I_d)(\theta)$\\

\par\noindent\rule{0.49\textwidth}{0.1pt}

\subsubsection{Distributed Parameter}
For reference, the ANN weights:\\

\begin{tabular}{| m{1.5cm} | m{1.5cm} | m{1.5cm} | m{1.5cm} |}
    \hline
    \textbf{Index} & \textbf{Layer} & \textbf{Is Bias?}& \textbf{Shape}\\
    \hline
    0 & Input & False & (1, 100)\\
    \hline
    1 & Input & True & (100)\\
    \hline
    2 & Hidden & False & (100, 100)\\
    \hline
    3 & Hidden & True & (100)\\
    \hline
    4 & Output & False & (100, 1)\\
    \hline
    5 & Output & True & (100)\\
    \hline
\end{tabular}\\~\\

Instead of distributing the weights of the ANN as a whole, only the hidden weights (index 2) were chosen as the parameter to be distributed because it has 10000 out of the ANN's 10401 parameters, i.e. $\sim 96.145\%$ of the ANN's parameters. Moreover, distributing only one array of weights makes implementing and applying the prior and likelihood far easier, since prior and likelihood functions need to only work with only one array rather than a list of arrays with non-matching shapes.\\

\subsection{VI Implementation}
The implementation of VI is done using PyTorch and the package \textit{torchbnn} (Lee et al., 2021), which is an implementation of BNNs using PyTorch. The BNN architecture is given below:\\

\begin{tabular}{| m{1cm} | m{2cm} | m{1.5cm} | m{2cm} |}
    \hline
    \textbf{Layer} & \textbf{Type} & \textbf{Shape} & \textbf{Parameters}\\
    \hline
    Input & Linear & (1, 100) & 200\\
    \hline
    Hidden & Bayes' Linear & (100, 100) & 10100\\
    \hline
    Output & Linear & (100, 1) & 101\\
    \hline
\end{tabular}\\~\\

For parity in the architectures of the HMC implementation and the VI implementation, only hidden layer's weights are to be distributed. More precisely, only the hidden layer is a Bayes' linear layer. In \textit{torchbnn}, a Bayes' linear layer is one with a normal prior $\mathcal{N}(\mu, \sigma)$ and with a normal variational distribution $q_\phi$ parameterised by $\phi = (\mu_q, \sigma_q)$, wherein $\mu_q$ and $\sigma_q$ are the mean and standard deviation of the layer's weights.\\

The model is trained using mean squared error (MSE) for the point estimation loss and KL-divergence between the variational distribution and the prior. The sum of the MSE and KL-divergence serves as a measure of distance between the variational distribution and the posterior. The model is trained for $3000$ epochs with a batch size of $100$. As with HMC BNN, to improve the VI BNN's performance, (1) per training loop, only the weights of the epoch with the lowest loss are saved, and (2) $5$ training loops are run for the same regression problem, with the weights reinitialised at the start of every loop. In the end, the best performing weights (i.e. the weights with the lowest loss) are chosen

\section{Results}

\begin{table}[H]
\centering
\begin{tabular}{c}
\includegraphics[width=0.5\textwidth]{images/hmc_bnn--problem_A--sb.png}\\
\includegraphics[width=0.5\textwidth]{images/hmc_bnn--problem_B--sb.png}\\
\includegraphics[width=0.5\textwidth]{images/hmc_bnn--problem_C--sb.png}\\
\includegraphics[width=0.5\textwidth]{images/hmc_bnn--problem_D--sb.png}\\
\end{tabular}
\caption{HMC BNN Performance}
\label{tbl:table_of_figures}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{c}
\includegraphics[width=0.5\textwidth]{images/vi_bnn--problem_A--sb.png}\\
\includegraphics[width=0.5\textwidth]{images/vi_bnn--problem_B--sb.png}\\
\includegraphics[width=0.5\textwidth]{images/vi_bnn--problem_C--sb.png}\\
\includegraphics[width=0.5\textwidth]{images/vi_bnn--problem_D--sb.png}\\
\end{tabular}
\caption{VI BNN Performance}
\label{tbl:table_of_figures}
\end{table}


\begin{thebibliography}{00}

\bibitem{Andrieu and Thoms, 2008} Andrieu, C. and Thoms, J. (2008). A tutorial on adaptive MCMC. \textit{Springer}. 18, pp. 343–373. doi:10.1007/s11222-008-9110-y.

\bibitem{Betancourt, 2018} Betancourt, M. (2018). A Conceptual Introduction to Hamiltonian Monte Carlo. \textit{arXiv preprint arXiv:1701.02434}.

\bibitem{Brian and Da Veiga, 2022} Brian, S. and Da Veiga, S. (2022). Benchmarking Bayesian neural networks and evaluation metrics for regression tasks. \textit{arXiv preprint arXiv:2206.06779}.

\bibitem{Carroll, 2019} Carroll, C. (2019). \textit{Hamiltonian Monte Carlo from scratch}. [online] Available from: \url{https://colindcarroll.com/2019/04/11/hamiltonian-monte-carlo-from-scratch} [Accessed 21 August 2024].

\bibitem{Chandra and Simmons, 2023} Chandra, R. and Simmons, J. (2023). Bayesian neural networks via MCMC: a Python-based tutorial. \textit{arXiv preprint arXiv:2304.02595}.

\bibitem{Foong et al., 2019} Foong, A. Y. K., Burt, D. R., Li, Y. and Turner, R. E. (2019). On the Expressiveness of Approximate Inference in Bayesian Neural Networks. \textit{arXiv preprint arXiv:1909.00719}.

\bibitem{Ganguly and Earp, 2021} Ganguly, A. and Earp, S. W. F. (2021). An Introduction to Variational Inference. \textit{arXiv preprint arXiv:2108.13083}.

\bibitem{janosh.dev, 2019} janosh.dev (2019). \textit{Training BNNs with HMC}. [online] Available from: \url{https://janosh.dev/posts/hmc-bnn} [Accessed 21 August 2024].

\bibitem{Jospin et al., 2020} Jospin, L. V., Laga, H., Boussaid, F., Buntine, W. and Bennamoun, M. (2020). Hands-on Bayesian Neural Networks – A Tutorial for Deep Learning Users. \textit{arXiv preprint arXiv:2007.06823}.

\bibitem{Kim, 2020} Kim, H. (2020). \textit{Bayesian-Neural-Network-Pytorch}. [online] Available from: \url{https://github.com/Harry24k/bayesian-neural-network-pytorch} [Accessed 21 August 2024].

\bibitem{Lee et al., 2021} Lee, S., Kim, H. and Lee, J. (2021). GradDiv: Adversarial Robustness of Randomized Neural Networks via Gradient Diversity Regularization. \textit{arXiv preprint arXiv:2107.02425}.

\bibitem{Martin et al., 2021} Martin. O. A., Kumar, R. and Lao, J. Bayesian Modeling and Computation in Python. Boca Ratón, Florida, USA: CRC Press, Taylor \& Francis Group. ISBN: 978-0-367-89436-8.

\bibitem{Murphy, 2012} Murphy, K. P. (2012). '3. Generative models for discrete data'. \textit{Machine Learning: A Probabilistic Perspective}. London, England: The MIT Press. pp. 65-87.

\bibitem{Neal, 2011} Neal, R. (2011). 'Chapter 5: MCMC using Hamiltonian dynamics'. In: Brooks, S., Gelman, A., Jones, G., and Meng, X. \textit{Handbook of Markov Chain Monte Carlo}. Boca Ratón, Florida, USA: CRC Press, Taylor \& Francis Group. pp. 113-162.

\bibitem{TensorFlow, 2023} TensorFlow. (2023). \textit{tfp.mcmc.SimpleStepSizeAdaptation}. [online] Available from: \url{https://www.tensorflow.org/probability/api_docs/python/tfp/mcmc/SimpleStepSizeAdaptation} [Accessed 21 August 2024].

\bibitem{Yao et al., 2019} Yao, J., Pan, W., Ghosh, S. and Doshi-Velez, F. (2019). Quality of Uncertainty Quantification for Bayesian Neural Network Inference. \textit{arXiv preprint arXiv:1906.09686}.

\end{thebibliography}
\vspace{12pt}

\clearpage %Page break

\section*{Appendix}

\par\noindent\rule{0.49\textwidth}{0.1pt}

\subsection*{Appendix A: Use of Negative Log-Probability in HMC}
Let $\theta = (\theta_1, \theta_2 ... \theta_k)$ be a parameterisation of a given generative model. Let $m$ be the momentum, where $m = (m_1, m_2 ... m_k)$ (each $m_i$ being the momentum in the $i$th dimension). Also, note that $P(\theta, m) = P(\theta|D)P(m)$, which means $\log P(\theta, m) = \log (P(\theta|D) P(m)) = \log P(\theta|D) + \log P(m)$. Hence:

\begin{equation*}
	- \log P(\theta, m) = - \log P(\theta|D) - \log P(m)
\end{equation*}

This is of the form $H(\theta, m) = K(\theta) + V(\theta)$, where $H(\theta, m) = - \log P(\theta, m)$ (Hamiltonian),  $K(m) = - \log P(m)$ ("kinetic energy"), and $V(\theta) = - \log P(\theta|D)$ ("potential energy"). Thus, we have Hamilton's equations:

\begin{equation*}
	\frac{d \theta}{dt} = \frac{\delta H}{\delta m}, \text{ } \frac{dm}{dt} = - \frac{\delta H}{\delta \theta}
\end{equation*}

Hamilton's equations describe the state of a physical system with one body of a fixed mass moving in a space of $k$ dimensions ($k \geq 1$). Now, even though Hamilton's equations were meant to deal with physical spaces, we can accurately extend their application to analogous simulated or abstract spaces, such as a high-dimensional parameter space of a Bayesian model.\\

In the context BI, the "position" of the sample can be thought of as the location of a particle, and the "momentum" provides the force needed to move the particle through the parameter space of the distributions. As shown above, the Hamiltonian for BI has the negative log-probability of the posterior. However, since the negative log-probability preserves the distribution of the posterior's probability mass as well as the posterior's modes (albeit that the modes are represented by minima instead of maxima), samples from the the negative log-probability of the posterior follow the same distribution as samples the posterior. Hence, Hamilton's equations a valid means to explore the posterior.

\par\noindent\rule{0.49\textwidth}{0.1pt}

\subsection*{Appendix B: Reasoning for HMC's Acceptance Criterion}
Let $\theta$ be the current position (i.e. the current state of the Markov chain sampling the Bayesian model's parameterisations). Let $m$ be the current momentum. Likewise, let $\theta^*$ and $m^*$ be the proposed position and momentum. Also, let $H(\theta, m)$ be the Hamiltonian for position $\theta$ and momentum $m$. Then, we have the following cases:\\

\begin{itemize}
	\item $e^{H(\theta, m) - H(\theta^*, m^*)} \geq 1 \implies H(\theta^*, m^*) \geq H(\theta, m)$
	\item $e^{H(\theta, m) - H(\theta^*, m^*)} \leq 1 \implies H(\theta^*, m^*) \leq H(\theta, m)$\\
\end{itemize}

$H(\theta^*, m^*) \geq H(\theta, m)$ means the proposed state is from a region in the posterior of equal or higher probability mass, which means it should always be accepted, because we want to sample more from regions of equal or higher probability mass. $H(\theta^*, m^*) < H(\theta, m)$ means the proposed state is from a region of lower probability mass, which means it should be accepted only probabilistically, with the probability of accepting it being proportional to its closeness to the current state (in terms of probability density), because we want there to be a lower but non-zero chance of sampling from a sparser region after sampling from a denser region, with the condition that the lower the density, the lower the chance.

\par\noindent\rule{0.49\textwidth}{0.1pt}

\subsection*{Appendix C: VI with Evidence Lower Bound (ELBO)}
KL-divergence is a well-established measure of distance used in VI. If used, the optimisation function would based on the KL-divergence between $q_\phi(\theta)$ and $p = P(\theta|D)$. Given that $\Theta$ is the set of all hypothetical parameterisations of a given model, the KL-divergence between $q_\phi(\theta)$ and $P(\theta|D)$ is given by the following:

\begin{equation*}
	KL(q_\phi(\theta) || P(\theta|D))
	= \int_{\theta \in \Theta} q_\phi(\theta) \log \frac{q_\phi(\theta)}{P(\theta|D)} d\theta
\end{equation*}\\

However, $P(\theta|D)$ is the posterior being approximated and thus cannot be in the optimisation function. But there exists a function derived from KL-divergence, namely the Evidence Lower Bound (ELBO), that uses only the variational distribution $q_\phi$ and the joint distribution $P(\theta, D) = P(D|\theta)|P(\theta)$, i.e. the unnormalised posterior. Note that these distributions are known, since $q_\phi$ and the prior $P(\theta)$ are chosen and since the likelihood $P(D|\theta)$ is based on a chosen generative model. Mathematically, ELBO is given as follows (Jospin et al., 2020):

\begin{equation*}
	\int_{\theta \in \Theta} q_\phi(\theta) \log \frac{p(\theta, D)}{q_\phi(\theta)} d\theta
\end{equation*}\\

ELBO is derived such that maximising it achieves the same optimisation as minimising KL-divergence. In practice, the above integral is estimated through numerical methods, e.g. averaging Monte Carlo samples drawn from the variational and joint distributions and plugging them into the ELBO formula\footnote{Here, the integral becomes a summation.} (Martin et al., 2021). For the derivation of ELBO, check \href{https://github.com/pranigopu/mastersProject/tree/main/conceptual-notes/bayesian-inference/sampling-methods/variational-inference-vi}{\textit{conceptual-notes/bayesian-inference/sampling-methods/variational-inference-vi}} in \href{https://github.com/pranigopu/mastersProject}{\textit{github.com/pranigopu/mastersProject}}, the repository associated with this paper.

\end{document}


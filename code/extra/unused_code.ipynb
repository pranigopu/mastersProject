{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "feff7a10-af5f-42eb-ad26-371aa3dd530c",
   "metadata": {},
   "source": [
    "# From `mcmc_for_neural_networks.ipynb`\n",
    "Code previously used in `mcmc_for_neural_networks` to plot the results for section 1..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873fa682-5a86-462e-a75f-96d2d0cccf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the plot size and font size:\n",
    "fontsize = 15\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "\n",
    "# Plotting the estimated, i.e. sampled posterior:\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.hist(plot_points,  bins = 20, color='C0', alpha=0.7, label='Sampled Posterior')\n",
    "ax1.set_xlim(ax1.get_xlim())\n",
    "\n",
    "# Plotting the true posterior:\n",
    "ax2 = ax1.twinx()\n",
    "ax2.grid(False)\n",
    "ax2.plot(true_posterior[:,0], true_posterior[:,1], linewidth=2, color='C1', label='True Posterior')\n",
    "ax2.set_ylim(0, ax2.get_ylim()[1])\n",
    "ax2.set_ylabel('Density', fontsize = fontsize)\n",
    "\n",
    "# Final plot display:\n",
    "ax1.set_ylabel('Frequency', fontsize = fontsize, labelpad=10)\n",
    "ax1.set_xlabel('Parameter value', fontsize = fontsize, labelpad=10)\n",
    "ax1.set_title('Posterior', fontsize = fontsize, pad=10)\n",
    "lgd = plt.legend(bbox_to_anchor=(1.25,0.5), loc='center left')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af329872-e286-47bf-a8a4-8b2eb864e550",
   "metadata": {},
   "source": [
    "# From `hamiltonian_monte_carlo.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33690ccb-6d74-438a-94a2-23a2b14db64c",
   "metadata": {},
   "source": [
    "# Basic Application\n",
    "**_For more theoretical details about the example problem used here, see_** [\"1. Practical Implementation\" from `mcmc_for_neural_networks.ipynb`](https://github.com/pranigopu/mastersProject/tree/420f7b4e1c4d5a65fc1f0d94d013c4b0d2a53fc3/code#1.-Practical-Introduction-to-MCMC).\n",
    "\n",
    "---\n",
    "\n",
    "**General problem statement**:\n",
    "\n",
    "Sampling the posterior of a single parameter (we shall call it the \"model parameter\").\n",
    "\n",
    "**Specific problem statement**:\n",
    "\n",
    "Obtain information about the probability of success $\\theta$ (which is our model parameter) given some data of $k$ successes in $n$ trials using an uninformative prior (e.g. a uniform distribution, i.e. before making observations, we assume that $\\theta$ is uniformly distributed). Here, the outcomes of the real-life process being modelled are distributed by a binomial distribution with some probability of success, which is our target distribution. Note that $\\theta$ is the probability of success of our model, and we try to find the target distribution by making our model as similar as possible to the real-life process. In other words, we shall solve for the posterior distribution of parameter $\\theta$, and thereby find (or at least estimate) the target distribution.\n",
    "\n",
    "**NOTE**: $\\theta$ _is the model's <u>probability</u> of success, which means its uniform prior must be_ $\\text{Uniform}([0, 1])$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b45672e7-d1c4-43ce-81b5-668434f137e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from numpy import random\n",
    "from tqdm import tqdm # NOTE: `tqdm` is just used to display the progress bar when looping over iterables\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b42754-a482-412a-bc21-e937fe9edd7b",
   "metadata": {},
   "source": [
    "## Defining the prior and likelihood\n",
    "By Bayes' rule, we know that:\n",
    "\n",
    "$p = P(\\theta|D) \\propto P(D|\\theta) P(\\theta)$\n",
    "\n",
    "Our prior has already been defined as $\\text{Uniform}([0, 1])$ (see the previous sections).\n",
    "\n",
    "---\n",
    "\n",
    "Hence, we need to define the likelihood function given our data $(k, n)$. How? Firstly, we know it must be a binomial distribution, since the likelihood is the probability that our model would generate the outcomes $D$ (generated by the real-life process), given that the model parameterised by $\\theta$. Hence, the likelihood function is a function that inputs $k$ (the number of successes), $n$ (the number of trials) and $\\theta$ (the probability of success) and outputs the probability mass of $(k, n)$ under $\\text{Binomial}(\\theta)$, i.e. the binomial distribution parameterised by $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51b95e47-0715-4945-aedc-75c48e8b8d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define our likelihood function which will be dependent on provided `data`\n",
    "likelihood_function = lambda k, n, θ: stats.binom.pmf(k, n, θ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922ea27d-5dc6-49a7-8e37-dc2f43a93465",
   "metadata": {},
   "source": [
    "**KEY PRACTICAL NOTE FOR THIS EXAMPLE**:\n",
    "\n",
    "Since we use an uinformative uniform prior $\\text{Uniform}([0, 1])$, where $P(\\theta) = 1$ if and only if $\\theta \\in [0, 1]$.\n",
    "\n",
    "$\\implies P(D|\\theta) P(\\theta) = P(D|\\theta)$\n",
    "\n",
    "However, in general (apart from this example), we should also define the prior function and use $P(D|\\theta) P(\\theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797d8618-c1a5-433e-a965-e8741303c0b8",
   "metadata": {},
   "source": [
    "## Setting up MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2aaebfb-c2a0-4b0e-b846-f2cb5a2fc6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic MCMC parameters:\n",
    "n_samples = 10000 # number of samples to draw from the posterior\n",
    "burnin = 2500 # number of samples to discard before recording draws from the posterior\n",
    "\n",
    "# Specifying the data, ensuring k <= n:\n",
    "binom_k = 80 # Number of successes\n",
    "binom_n = 100 # Number of trials\n",
    "\n",
    "# Initial parameter value from which to start sampling (drawn from the prior):\n",
    "θ_current = random.uniform(0, 1)\n",
    "\n",
    "# Variable to keep track of the number of accepted samples:\n",
    "n_accepted_samples = 0\n",
    "\n",
    "# Create an array of NaNs to fill with our samples:\n",
    "p_posterior = np.full(n_samples, np.nan) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056e420c-8e63-44aa-a6e0-24be8b72f0fe",
   "metadata": {},
   "source": [
    "## Running MCMC using HMC algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c218d42d-def7-42d7-9b9a-8cc3c489b6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_log_prob(θ):\n",
    "    return -1*np.log(likelihood_function(binom_k, binom_n, θ))\n",
    "\n",
    "def hmc(θ_current, path_len=1, step_size=0.25):\n",
    "    # Setup:\n",
    "    steps = int(path_len/step_size) # `path_len` and `step_size` are tricky parameters to tune\n",
    "    momentum_dist = stats.norm(0, 1)\n",
    "    \n",
    "    # Generate samples:\n",
    "    θ_proposed = np.copy(θ_current)\n",
    "    m_current = momentum_dist.rvs()        \n",
    "    m_proposed = np.copy(m_current) \n",
    "    \n",
    "    # Gradient of PDF w.r.t. position (θ_current) a.k.a. potential energy w.r.t. position:\n",
    "    δVδθ = binom_k*θ_current**(binom_k-1) * (1-θ_current)**(binom_n-binom_k)\n",
    "    δVδθ += (θ_current**binom_k) * (-1*(binom_n-binom_k)*(1-θ_current)**(binom_n-binom_k-1))\n",
    "    δVδθ /= θ_current**binom_k * (1-θ_current)**(binom_n-binom_k)\n",
    "    δVδθ *= -1\n",
    "\n",
    "    # leapfrog integration begins...\n",
    "    for s in range(steps): \n",
    "        m_proposed += step_size*δVδθ/2 # As potential energy increases, kinetic energy decreases, half-step\n",
    "        θ_proposed += step_size*m_proposed # Position increases as function of momentum \n",
    "        m_proposed += step_size*δVδθ/2 # Second half-step \"leapfrog\" update to momentum    \n",
    "    # ... leapfrog integration ends\n",
    "    \n",
    "    m_proposed = -1*m_proposed # Flip momentum for reversibility     \n",
    "\n",
    "    return θ_proposed, m_current, m_proposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5941a9fc-385d-4125-ab9c-b8e3e3d2dd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 10000 MCMC samples from the posterior:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████| 10000/10000 [00:18<00:00, 528.54it/s]\n"
     ]
    }
   ],
   "source": [
    "print('Generating {} MCMC samples from the posterior:'.format(n_samples))\n",
    "for i in tqdm(np.arange(n_samples), ncols=100):\n",
    "    # Sample a value uniformly from 0 to 1 as a proposal:\n",
    "    θ_proposed, m_current, m_proposed = hmc(θ_current)\n",
    "\n",
    "    #------------------------------------\n",
    "    # Calculate the Metropolis-Hastings acceptance probability α based on the prior and likelihood:\n",
    "    θ_current_neg_log_prob = neg_log_prob(θ_current)\n",
    "    θ_proposed_neg_log_prob = neg_log_prob(θ_proposed)\n",
    "    \n",
    "    m_current_neg_log_prob = neg_log_prob(m_current)\n",
    "    m_proposed_neg_log_prob = neg_log_prob(m_proposed)\n",
    "    \n",
    "    # Account for negatives and log (probabilties):\n",
    "    target = θ_current_neg_log_prob - θ_proposed_neg_log_prob # -P(θ_proposed)/P(θ_current)\n",
    "    adjustment = m_proposed_neg_log_prob - m_current_neg_log_prob # P(m_current)/P(m_proposed)\n",
    "    log_α = target + adjustment # Logarithm of acceptance probability\n",
    "    # NOTE: Acceptance probability is given by [P(θ_proposed)*P(m_current)]/[P(θ_current)*P(m_proposed)] \n",
    "    \n",
    "    #------------------------------------\n",
    "    # Probabilistically accepting the proposal:\n",
    "    if np.log(random.uniform(0, 1)) < log_α: # Simulates the probability of acceptance as α\n",
    "        θ_current = θ_proposed # then update the current sample to the propoal for the next iteration\n",
    "        n_accepted_samples += 1 # add to the count of accepted samples\n",
    "\n",
    "    # Store the current sample\n",
    "    p_posterior[i] = θ_current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a041305-7784-477f-b9cb-4d67f3b1ce39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of accepted samples: 0.000 %\n",
      "Mean value of the estimated posterior (excluding burn-in): 0.642\n"
     ]
    }
   ],
   "source": [
    "per_accept = (n_accepted_samples/n_samples)*100\n",
    "print(f'Number of accepted samples: {per_accept:.3f} %')\n",
    "posterior_mean = np.mean(p_posterior[burnin:])\n",
    "print(f'Mean value of the estimated posterior (excluding burn-in): {posterior_mean:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2684e8d-aa41-4f68-98df-d70b0709e952",
   "metadata": {},
   "source": [
    "# From `bnn_basics.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "985ab5ca-630b-4f56-9bb4-f7883fa7996e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(1, 100, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 1, bias=True))\n",
    "\n",
    "    #================================================\n",
    "    # Initialising weights according to a standard normal distribution:\n",
    "    def init_weights(self, layer):\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            torch.nn.init.normal_(layer.weight)\n",
    "    \n",
    "    #================================================\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    #================================================\n",
    "    def get_flattened_weights(self):\n",
    "        flattened_weights = torch.tensor([])\n",
    "        for layer in self.model:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                flattened_weights = torch.concat((flattened_weights, torch.flatten(layer.weight)))\n",
    "        return torch.detach(flattened_weights)\n",
    "        '''\n",
    "        Why use `torch.detach` and not simply return the flattened weights? See implementation notes below.\n",
    "        '''\n",
    "    \n",
    "    #================================================\n",
    "    def set_weights_from_flattened_weights(self, θ):\n",
    "        '''\n",
    "        Set weights from proposed flattened weights.\n",
    "        ------------------------------------\n",
    "        θ : Flattened vector of proposed model parameters\n",
    "        '''\n",
    "        \n",
    "        for layer in self.model:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                # Obtaining the size of the layer (i.e. the number of weights in the layer):\n",
    "                size = int(torch.prod(torch.tensor(layer.weight.shape)))\n",
    "                # NOTE 1: In our case, the above amounts to doing `int(layer.weight.shape[0] * layer.weight.shape[1])`\n",
    "                # NOTE 2: Coverting `size` to an integer is necessary, since we want to use `size` for slicing\n",
    "\n",
    "                # Setting the layer's weights and removing assigned weights from θ:\n",
    "                with torch.no_grad():\n",
    "                    print('HAHA', θ)\n",
    "                    layer.weight = nn.Parameter(torch.reshape(θ[:size], shape=layer.weight.shape))\n",
    "                '''\n",
    "                REFERENCE FOR THE ABOVE: https://discuss.pytorch.org/t/how-to-manually-set-the-weights-in-a-two-layer-linear-model/45902\n",
    "                '''\n",
    "                if size < len(θ):\n",
    "                    θ = θ[size:]\n",
    "\n",
    "    #================================================\n",
    "    # Function to take in the data and the proposed sample of the parameter and return the prediction:\n",
    "    def evaluate_proposal(self, D, θ):\n",
    "        '''\n",
    "        Apply the proposed parameters and then use the model to predict.\n",
    "        ------------------------------------\n",
    "        D : Array of data points\n",
    "        θ : Flattened vector of proposed model parameters\n",
    "\n",
    "        NOTE: Although θ is a vector of parameters, as a vector, it is collectively regarded as one parameter.\n",
    "        '''\n",
    "\n",
    "        # Setting model's weights to the proposed weights:\n",
    "        self.set_weights_from_flattened_weights(θ)\n",
    "        \n",
    "        # Predict based on the newly parameterised model:\n",
    "        prediction = self.forward(D)\n",
    "        return prediction\n",
    "\n",
    "    #================================================\n",
    "    def train(self, x, y, n_epochs=500):\n",
    "        # Initialising MSE part of the loss function:\n",
    "        mse_loss = nn.MSELoss()\n",
    "\n",
    "        # Initialising the optimiser used for gradient descent:\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "        #------------------------------------\n",
    "        # Training loop:\n",
    "        \n",
    "        for step in range(n_epochs):\n",
    "            # Obtaining the model's predicted values for the inputs:\n",
    "            prediction = self.model(x)\n",
    "        \n",
    "            # Calculating the loss function:\n",
    "            mse = mse_loss(prediction, y)\n",
    "            \n",
    "            # Optimisation step:\n",
    "            optimizer.zero_grad() # Resetting the optimiser's gradients\n",
    "            mse.backward()       # Calculating the gradients for backpropagation\n",
    "            optimizer.step()      # Applying the gradients for backpropagation\n",
    "        \n",
    "        print('MSE : %2.2f' % (mse.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5081e7bf-e50d-44af-9b5e-1c6911e041af",
   "metadata": {},
   "source": [
    "**IMPLEMENTATION NOTE: `.train`**:\n",
    "\n",
    "The training function defined above is a traditional training loop, without any Bayesian elements.\n",
    "\n",
    "---\n",
    "\n",
    "**IMPLEMENTATION NOTE: Why use `torch.detach` and not simply return the flattened weights?**:\n",
    "\n",
    "In later processes, we would need to apply `.numpy` (i.e. extracting the tensor's values as a NumPy array) to our tensor of flattened weights. However, since the flattened weights were obtained from the ANN layers' parameters, which require `grad` (i.e. automatic gradient calculation is enabled for them), the returned tensor of flattened weights also requires grad. Thus, when applying `.numpy` to our tensor of flattened weights, we would get the following error message:\n",
    "\n",
    "```\n",
    "RuntimeError: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.\n",
    "```\n",
    "\n",
    "To avoid this, we use `torch.detach` on the tensor of flattened weights before returning it. `torch.detach` is a method that returns a new tensor, detached from the current graph (i.e. the [computational graph](https://pytorch.org/blog/computational-graphs-constructed-in-pytorch/) by means of which the partial gradient of a function can be calculated with respect to the tensor). The result will never require gradient, which is what we need.\n",
    "\n",
    "> **Reference**: [`torch.Tensor.detach` (PyTorch documentation)](https://pytorch.org/docs/stable/generated/torch.Tensor.detach.html)\n",
    "\n",
    "---\n",
    "\n",
    "**END OF IMPLEMENTATION NOTES**\n",
    "\n",
    "---\n",
    "\n",
    "Some code for testing the weight flattening and assignment functions (you can copy-paste the code and run it for yourself)...\n",
    "\n",
    "```python\n",
    "# Initialising the model:\n",
    "model = ANN()\n",
    "\n",
    "# Obtaining new flattened weights:\n",
    "w = torch.ones_like(model.get_flattened_weights()) # A tensor of ones with the same size as `model.get_flattened_weights()`\n",
    "w[:50] *= 2\n",
    "w[50:150] *= 3\n",
    "\n",
    "# Setting the new flattened weights to the model:\n",
    "model.set_weights_from_flattened_weights(w)\n",
    "\n",
    "# Printing the model's updated layer weights:\n",
    "for layer in model.model:\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        print(layer.weight)\n",
    "```\n",
    "\n",
    "By running the above code, I have verified the correctness of the weight flattening and setting functions.\n",
    "\n",
    "---\n",
    "\n",
    "Training the ANN with the traditional training loop to test the ANN's functions (chiefly weight initialisation and training)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf68acb-337c-4b12-872c-0b109ea5aaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ANN()\n",
    "\n",
    "# Initialising weights:\n",
    "model.apply(model.init_weights)\n",
    "# NOTE: This model shall be our regression function\n",
    "\n",
    "# Training model:\n",
    "model.train(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dc3ad2-b12b-40ae-9e83-8742724962f7",
   "metadata": {},
   "source": [
    "**IMPLEMENTATION NOTE: `torch.nn.Module.apply(fn)`**:\n",
    "\n",
    "Applies `fn` recursively to every submodule (as returned by `.children()`) as well as self. Typical use includes initialising the parameters of a model (for more on initialising module parameters, see:[`nn-init-doc`](https://pytorch.org/docs/stable/nn.init.html)).\n",
    "\n",
    "> **Reference**: [Source code for `torch.nn.modules.module` (PyTorch documentation)](https://pytorch.org/docs/master/_modules/torch/nn/modules/module.html#Module.apply)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "feff7a10-af5f-42eb-ad26-371aa3dd530c",
   "metadata": {},
   "source": [
    "# From `mcmc_for_neural_networks.ipynb`\n",
    "Code previously used in `mcmc_for_neural_networks` to plot the results for section 1..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873fa682-5a86-462e-a75f-96d2d0cccf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the plot size and font size:\n",
    "fontsize = 15\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "\n",
    "# Plotting the estimated, i.e. sampled posterior:\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.hist(plot_points,  bins = 20, color='C0', alpha=0.7, label='Sampled Posterior')\n",
    "ax1.set_xlim(ax1.get_xlim())\n",
    "\n",
    "# Plotting the true posterior:\n",
    "ax2 = ax1.twinx()\n",
    "ax2.grid(False)\n",
    "ax2.plot(true_posterior[:,0], true_posterior[:,1], linewidth=2, color='C1', label='True Posterior')\n",
    "ax2.set_ylim(0, ax2.get_ylim()[1])\n",
    "ax2.set_ylabel('Density', fontsize = fontsize)\n",
    "\n",
    "# Final plot display:\n",
    "ax1.set_ylabel('Frequency', fontsize = fontsize, labelpad=10)\n",
    "ax1.set_xlabel('Parameter value', fontsize = fontsize, labelpad=10)\n",
    "ax1.set_title('Posterior', fontsize = fontsize, pad=10)\n",
    "lgd = plt.legend(bbox_to_anchor=(1.25,0.5), loc='center left')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af329872-e286-47bf-a8a4-8b2eb864e550",
   "metadata": {},
   "source": [
    "# From `hamiltonian_monte_carlo.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33690ccb-6d74-438a-94a2-23a2b14db64c",
   "metadata": {},
   "source": [
    "# Basic Application\n",
    "**_For more theoretical details about the example problem used here, see_** [\"1. Practical Implementation\" from `mcmc_for_neural_networks.ipynb`](https://github.com/pranigopu/mastersProject/tree/420f7b4e1c4d5a65fc1f0d94d013c4b0d2a53fc3/code#1.-Practical-Introduction-to-MCMC).\n",
    "\n",
    "---\n",
    "\n",
    "**General problem statement**:\n",
    "\n",
    "Sampling the posterior of a single parameter (we shall call it the \"model parameter\").\n",
    "\n",
    "**Specific problem statement**:\n",
    "\n",
    "Obtain information about the probability of success $\\theta$ (which is our model parameter) given some data of $k$ successes in $n$ trials using an uninformative prior (e.g. a uniform distribution, i.e. before making observations, we assume that $\\theta$ is uniformly distributed). Here, the outcomes of the real-life process being modelled are distributed by a binomial distribution with some probability of success, which is our target distribution. Note that $\\theta$ is the probability of success of our model, and we try to find the target distribution by making our model as similar as possible to the real-life process. In other words, we shall solve for the posterior distribution of parameter $\\theta$, and thereby find (or at least estimate) the target distribution.\n",
    "\n",
    "**NOTE**: $\\theta$ _is the model's <u>probability</u> of success, which means its uniform prior must be_ $\\text{Uniform}([0, 1])$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b45672e7-d1c4-43ce-81b5-668434f137e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from numpy import random\n",
    "from tqdm import tqdm # NOTE: `tqdm` is just used to display the progress bar when looping over iterables\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b42754-a482-412a-bc21-e937fe9edd7b",
   "metadata": {},
   "source": [
    "## Defining the prior and likelihood\n",
    "By Bayes' rule, we know that:\n",
    "\n",
    "$p = P(\\theta|D) \\propto P(D|\\theta) P(\\theta)$\n",
    "\n",
    "Our prior has already been defined as $\\text{Uniform}([0, 1])$ (see the previous sections).\n",
    "\n",
    "---\n",
    "\n",
    "Hence, we need to define the likelihood function given our data $(k, n)$. How? Firstly, we know it must be a binomial distribution, since the likelihood is the probability that our model would generate the outcomes $D$ (generated by the real-life process), given that the model parametrised by $\\theta$. Hence, the likelihood function is a function that inputs $k$ (the number of successes), $n$ (the number of trials) and $\\theta$ (the probability of success) and outputs the probability mass of $(k, n)$ under $\\text{Binomial}(\\theta)$, i.e. the binomial distribution parametrised by $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51b95e47-0715-4945-aedc-75c48e8b8d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define our likelihood function which will be dependent on provided `data`\n",
    "likelihood_function = lambda k, n, θ: stats.binom.pmf(k, n, θ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922ea27d-5dc6-49a7-8e37-dc2f43a93465",
   "metadata": {},
   "source": [
    "**KEY PRACTICAL NOTE FOR THIS EXAMPLE**:\n",
    "\n",
    "Since we use an uinformative uniform prior $\\text{Uniform}([0, 1])$, where $P(\\theta) = 1$ if and only if $\\theta \\in [0, 1]$.\n",
    "\n",
    "$\\implies P(D|\\theta) P(\\theta) = P(D|\\theta)$\n",
    "\n",
    "However, in general (apart from this example), we should also define the prior function and use $P(D|\\theta) P(\\theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797d8618-c1a5-433e-a965-e8741303c0b8",
   "metadata": {},
   "source": [
    "## Setting up MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2aaebfb-c2a0-4b0e-b846-f2cb5a2fc6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic MCMC parameters:\n",
    "n_samples = 10000 # number of samples to draw from the posterior\n",
    "burnin = 2500 # number of samples to discard before recording draws from the posterior\n",
    "\n",
    "# Specifying the data, ensuring k <= n:\n",
    "binom_k = 80 # Number of successes\n",
    "binom_n = 100 # Number of trials\n",
    "\n",
    "# Initial parameter value from which to start sampling (drawn from the prior):\n",
    "θ_current = random.uniform(0, 1)\n",
    "\n",
    "# Variable to keep track of the number of accepted samples:\n",
    "n_accepted_samples = 0\n",
    "\n",
    "# Create an array of NaNs to fill with our samples:\n",
    "p_posterior = np.full(n_samples, np.nan) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056e420c-8e63-44aa-a6e0-24be8b72f0fe",
   "metadata": {},
   "source": [
    "## Running MCMC using HMC algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c218d42d-def7-42d7-9b9a-8cc3c489b6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_log_prob(θ):\n",
    "    return -1*np.log(likelihood_function(binom_k, binom_n, θ))\n",
    "\n",
    "def hmc(θ_current, path_len=1, step_size=0.25):\n",
    "    # Setup:\n",
    "    steps = int(path_len/step_size) # `path_len` and `step_size` are tricky parameters to tune\n",
    "    momentum_dist = stats.norm(0, 1)\n",
    "    \n",
    "    # Generate samples:\n",
    "    θ_proposed = np.copy(θ_current)\n",
    "    m_current = momentum_dist.rvs()        \n",
    "    m_proposed = np.copy(m_current) \n",
    "    \n",
    "    # Gradient of PDF w.r.t. position (θ_current) a.k.a. potential energy w.r.t. position:\n",
    "    δVδθ = binom_k*θ_current**(binom_k-1) * (1-θ_current)**(binom_n-binom_k)\n",
    "    δVδθ += (θ_current**binom_k) * (-1*(binom_n-binom_k)*(1-θ_current)**(binom_n-binom_k-1))\n",
    "    δVδθ /= θ_current**binom_k * (1-θ_current)**(binom_n-binom_k)\n",
    "    δVδθ *= -1\n",
    "\n",
    "    # leapfrog integration begins...\n",
    "    for s in range(steps): \n",
    "        m_proposed += step_size*δVδθ/2 # As potential energy increases, kinetic energy decreases, half-step\n",
    "        θ_proposed += step_size*m_proposed # Position increases as function of momentum \n",
    "        m_proposed += step_size*δVδθ/2 # Second half-step \"leapfrog\" update to momentum    \n",
    "    # ... leapfrog integration ends\n",
    "    \n",
    "    m_proposed = -1*m_proposed # Flip momentum for reversibility     \n",
    "\n",
    "    return θ_proposed, m_current, m_proposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5941a9fc-385d-4125-ab9c-b8e3e3d2dd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 10000 MCMC samples from the posterior:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████| 10000/10000 [00:18<00:00, 528.54it/s]\n"
     ]
    }
   ],
   "source": [
    "print('Generating {} MCMC samples from the posterior:'.format(n_samples))\n",
    "for i in tqdm(np.arange(n_samples), ncols=100):\n",
    "    # Sample a value uniformly from 0 to 1 as a proposal:\n",
    "    θ_proposed, m_current, m_proposed = hmc(θ_current)\n",
    "\n",
    "    #------------------------------------\n",
    "    # Calculate the Metropolis-Hastings acceptance probability α based on the prior and likelihood:\n",
    "    θ_current_neg_log_prob = neg_log_prob(θ_current)\n",
    "    θ_proposed_neg_log_prob = neg_log_prob(θ_proposed)\n",
    "    \n",
    "    m_current_neg_log_prob = neg_log_prob(m_current)\n",
    "    m_proposed_neg_log_prob = neg_log_prob(m_proposed)\n",
    "    \n",
    "    # Account for negatives and log (probabilties):\n",
    "    target = θ_current_neg_log_prob - θ_proposed_neg_log_prob # -P(θ_proposed)/P(θ_current)\n",
    "    adjustment = m_proposed_neg_log_prob - m_current_neg_log_prob # P(m_current)/P(m_proposed)\n",
    "    log_α = target + adjustment # Logarithm of acceptance probability\n",
    "    # NOTE: Acceptance probability is given by [P(θ_proposed)*P(m_current)]/[P(θ_current)*P(m_proposed)] \n",
    "    \n",
    "    #------------------------------------\n",
    "    # Probabilistically accepting the proposal:\n",
    "    if np.log(random.uniform(0, 1)) < log_α: # Simulates the probability of acceptance as α\n",
    "        θ_current = θ_proposed # then update the current sample to the propoal for the next iteration\n",
    "        n_accepted_samples += 1 # add to the count of accepted samples\n",
    "\n",
    "    # Store the current sample\n",
    "    p_posterior[i] = θ_current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a041305-7784-477f-b9cb-4d67f3b1ce39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of accepted samples: 0.000 %\n",
      "Mean value of the estimated posterior (excluding burn-in): 0.642\n"
     ]
    }
   ],
   "source": [
    "per_accept = (n_accepted_samples/n_samples)*100\n",
    "print(f'Number of accepted samples: {per_accept:.3f} %')\n",
    "posterior_mean = np.mean(p_posterior[burnin:])\n",
    "print(f'Mean value of the estimated posterior (excluding burn-in): {posterior_mean:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

**EXPLORATORY ANALYSIS OF BAYESIAN MODELS**

---

**Contents**:

- [Introduction](#introduction)
- [Understanding model assumptions](#understanding-model-assumptions)
- [Understanding model predictions](#understanding-model-predictions)
- [Diagnosing numerical inference](#diagnosing-numerical-inference)
  - [The root of MCMC issues](#the-root-of-mcmc-issues)
  - [Diagnostic 1: Effective sample size (ESS)](#diagnostic-1-effective-sample-size-ess)
  - [Diagnostic 2: Potential scale reduction factor $\\hat{R}$](#diagnostic-2-potential-scale-reduction-factor-hatr)
- [Diagnostic 3: Monte Carlo standard error (MCSE)](#diagnostic-3-monte-carlo-standard-error-mcse)
- [Diagnostic 4: Plots](#diagnostic-4-plots)
- [Diagnostic 5: Divergences](#diagnostic-5-divergences)

---

> **Main resource**: [_2.Exploratory Analysis of Bayesian Models_ from **Bayesian Computation Book**](https://bayesiancomputationbook.com/markdown/chp_02.html)

---

**ABBREVIATIONS**:

- **iid**: Independently and identically distributed

---

# Introduction
Bayesian modelling is wider than inference; apart from specifying the model and calculating a posterior, there exist other tasks that are necessary for successful Bayesian data analysis. In particular successful Bayesian modelling approach may require:

- Diagnosing the quality of the inference results obtained using numerical methods
- Model criticism, including evaluations of both model assumptions and model predictions
- Comparison of models, including model selection or model averaging
- Preparation of the results for a particular audience

# Understanding model assumptions
Model assumptions are quantified using the prior. However, a problem in choosing priors is understanding their effect on how the model is updated using the data. For example, the choices made in the parameter space (e.g. choices about what parameters to define and/or what constraints to define for their possible values) may lead to something unexpected in the predicted data space (e.g. predicting absurd values with a higher-than-reasonable plausibility), which in turn may lead to the model updating less-than-optimally.

If we are not sure how well the prior encodes information relevant to the domain, we have a few alternatives: (1) We can rethink our model to incorporate further relevant information and insights. (2) We can use a prior that reduces (though does not necessarily eliminate) the chance of nonsensical results. (3) We can just fit the data and see if the data itself is informative enough to estimate a posterior that excludes nonsensical values. But how do we know what to do? Evidently, we must in some way evaluate our prior, i.e. our model's assumptions.

Since prior information and/or assumptions are only meaningful with respect to the domain, we need to put them in the context of a domain. Furthermore, thinking in terms of synthetic data generated by the pre-observational model (i.e. model before any observations are made) is generally easier than thinking in terms of the model's parameters. We can evaluate the parameter values by evaluating the synthetic data generated by the corresponding pre-observational models; such evaluation can be done using prior predictive distributions (PrPDs). PrPDs become even more useful for complex models where parameters are transformed in many ways or steps, or where many priors (i.e. prior information and/or assumptions) interact with each other.

**NOTE**: _The synthetic data (generated by the model) is data that reveals what the model expects for future outcomes. If the model is sufficiently accurate, then the synthetic data reveals the kind of data we may expect to actually observe._

Additionally, PrPDs can be used to present results or discuss models in a more intuitive way to a wide audience. For example, a domain expert may not be familiar with statistical notation or code and thus using those devices to communicate the model may not be productive, but if you show them the implications of one or more models in terms of synthetic data, there is more accessible and interpretable material to discuss. which can provide valuable insights both for the domain expert and yourself. Furthermore, computing the PrPD has other advantages, such as helping us debug models and ensuring that they are properly written and able to run in our computational environment.

# Understanding model predictions
Just as we can use synthetic data from the PrPD to help us inspect our model's efficacy in modelling the problem, we can perform a similar inspection using synthetic data generated by the post-observational model (i.e. the model after at least some observed data). Just as a pre-observational model is inspected using the PrPD, the post-observational model is inspected using the posterior predictive distribution (PPD).

---

**TERMINOLOGY CHECKPOINT**:

**Prior predictive checks (PrPC)**:

- Inspecting a model before any observations are made
- Inspection is done using the PrPD

**Posterior predictive checks (PPC)**:

- Inspecting a model conditioned on at least some observed data
- Inspection is done using the PPD

**NOTE**: _The pre-observational model is defined using the prior and likelihood alone, whereas the post-observational model is defined using the prior, the likelihood and the posterior. A "model" here represents a certain mapping between outcomes and probabilities._

---

The basic idea behind PPC is to evaluate how close the synthetic data generated by the post-observational model are to the observed data. Ideally, the way we assess closeness should be problem-dependent (i.e. based on problem-specific distance metric), but we may use more generalised distance metric too. We may even want to use more than one distance metric in order to assess different ways our model(s) match or mismatch with the data.

# Diagnosing numerical inference
**_Specifically, diagosing inference using Markov chain Monte Carlo methods_**

In cases where the posterior distribution cannot be evaluated mathematically, it must be evaluated numerically, i.e. using computation. Estimating the posterior distribution using numerical methods (i.e. sampling methods) is called **numerical inference**. Specifically, we focus on the most practically applicable methods, namely Markov chain Monte Carlo (MCMC) methods (see: ["Markov chain Monte Carlo" from _Sampling Methods_](https://github.com/pranigopu/mastersProject/blob/main/conceptual-notes/bayesian-inference/sampling-methods.md)). In diagnosing numerical inference, we want to (1) evaluate the quality of the sampling method, (2) identify issues with the sampling that could lead to poor estimation and (3) correct for the aforementioned issues.

## The root of MCMC issues
The root of issues when using MCMC methods is also the root of its strength, namely the fact that consequent samples are not uncorrelated, and each sample depends on the previous sample, as in a Markov chain. Hence, samples from MCMC methods always have some degree of autocorrelation; we shall soon see the issues arising from this fact.

## Diagnostic 1: Effective sample size (ESS)
**LEXICAL NOTE**:

"Sample" in general denotes a particular value that is drawn or observed from the distribution. It means something similar to an individual "draw". However, "sample" when talked about as a collection, especially when ascribed to a size (e.g. sample of size 10), denotes a collection of individual samples. Hence, "10 samples" = "sample of size 10"; in both cases, the term "sample" is used in a different sense. It is a bit confusing, but since I have seen the term "sample" used both ways, and since there is reason to use it both ways for different purposes, I wanted to make it clear how to identify the sense in which the term "sample" is used based on the context. To make my usage clearer, I shall use "draw a sample" for the first usage, and "obtain a sample" for the second usage.

---

Samples from MCMC methods always have some degree of autocorrelation, which means the actual amount of information (about the target distribution) contained in the samples are going to be less than information contained in the same number of iid samples. Therefore, when using MCMC sampling methods, it is impossible to infer from the number of samples alone whether the number of samples is large enough to estimate the target distribution with sufficient accuracy.

**NOTE**: _Sufficiency depends on our purpose; for example, the estimation of the distribution mean may be sufficient for some applications, while others may require the estimation of an index or some other statistic._

ESS is a value calculated for a certain sample of a certain size, and it gives the size of the iid sample that would have a similar effectiveness (i.e. similar information about the target distribution being estimated) as the given sample. Note that ESS is not only for MCMC samples, but it is useful for analysing MCMC samples since it takes the samples' autocorrelation into account. Thus, the ESS can be used to estimate effectiveness of either a given sample or a sampling method.

- ESS of sample 1 of size 5000 = 4000
- ESS of sample 2 of size 200 = 100
- ESS of sample 3 of size 100 = 10

Hence, we get that sample 1 is the most effective, with as much information contained in its 5000 samples as information contained in an iid sample with 4000 samples, In other words, sample 1 of size 5000 is equivalent to an iid sample of size 4000. Sample 2 is less effective, while sample 3 is the least effective (take the ratio between their ESS values and their sizes).

---

**For sampling method vs. for sample alone**:

Evidently, the value of ESS for a sample can only tell about the effectiveness of the sampling method used when we also consider the size of the sample; for example, ESS = 4000 for a sample of size 6000 indicates a sampling method of lower effectiveness compared to ESS = 4000 for a sample of size 5000 (note, however, that the random seed used for random sampling can have an effect too, which is why ESS is only an estimate of effectiveness). However, if you only want care about checking if a sample is effective enough, i.e. if it is equivalent to a sufficiently large iid sample, ESS alone is enough, since you do not care about the effectiveness of the sampling method itself.

---

**How the ESS can vary**:

**1. Random seed**:

If you rerun the sampling method that involves randomness in its sampling (as does any MCMC method), using a different random seed each time, you will see that the effective sample size you get will be different each time. In particular, for MCMC methods, where you start the Markov chain is key in deciding how long or short the burn-in will be (for the terminology, see: ["Markov chain Monte Carlo" from _Sampling Methods_](https://github.com/pranigopu/mastersProject/blob/main/conceptual-notes/bayesian-inference/sampling-methods.md)).

**2. Across parameter space**:

The effective sample size (ESS) for accurately estimating a high-density region of the target distribution is generally lower than the ESS for estimating a low-density region of the target distribution. This makes sense, because when sampling from a sampling method that at least tries to converge to the target distribution (e.g. MCMC methods), we are more likely to get higher-density samples than lower-density samples, which means we have sparser samples for estimating the exact shape of lower-density regions.

**NOTE**: _Hence, we can calculate ESS for particular quantiles of the distribution._

---

**Bulk-ESS**:

The ESS value for the centre of the distribution. Hence, it assesses how the centre of the distribution was resolved, rather than assessing the accuracy of the estimation of the whole distribution. As noted before, ESS can vary across the parameter space, making the bulk-ESS a practical ESS value for many use-cases.

## Diagnostic 2: Potential scale reduction factor $\hat{R}$
Under some general conditions, and given the right design of the Markov chain, MCMC methods have theoretical guarantees about the convergence of the sampling probabilities over time to the steady-state probabilities which follow the target distribution, irrespective of the starting point. However, the guarantees are only about convergence of the sampling probabilities over time, not necessarily about actually them actually reaching the desired values within a feasible number of steps.

Thus, in practice we need ways to estimate convergence for finite MCMC samples, i.e. we need to estimate how close are the sampling probabilities of the Markov chain to the sampling probabilities of samples drawn from the target distribution; evidently, we can only estimate the closeness, because we do not know the target distribution yet, since that is exactly what we are trying to estimate with the samples. One well-established idea is to run more than one Markov chain, starting from very different points (i.e. very different initial states, i.e. initial samples in our case) and then check the resulting chains to see if they look similar to each other. This notion can be formalised into a numerical diagnostic known as $\hat{R}$.

---

$\hat{R}$ **as potential scale reduction factor of variance**:

This is the original interpretation of $\hat{R}$. Under this interpretation, $\hat{R}$ is the limit of the factor by which the variance of the target distribution's estimation (done using sampling) is reduced, as the number of samples tends to infinity. It has a target value of 1, where $\hat{R} = 1$ means that increasing the number of samples will not reduce the variance of the estimation in the long-term.

**NOTE**: $\hat{R}$ _is essentially calculated for the given sampling method, using multiple finite samples obtained using this method._

---

**Calculation of** $\hat{R}$:

I shall not go into details. But in principle, $\hat{R}$ is a measure of the variance of all samples from all the different Markov chains in relation to the variance within each Markov chain. Ideally we should get a value of 1, as the variance between different Markov chains should be the same as the variance within each Markov chain. The idea is that if the sampling method is effective and reliable, the long-range variance of our estimates should not be that different, no matter where we start in the parameter space (i.e. no matter what the initial state — initial sample in our case — of our Markov chain is). Practically, values of $\hat{R} \leq 1.01$ are considered safe. Generally, $\hat{R}$ is higher than 1, and the aim is to make it as low as possible, i.e. as close to 1 as possible.

# Diagnostic 3: Monte Carlo standard error (MCSE)
MCSE for sample makes use of the Markov chain central limit theorem as well as the effective sample size (ESS) to derive an error value between expected sample values and observed sample values (observed in the Markov chain used in MCMC).

---

**KEY POINT 1**: Unlike ESS and $\hat{R}$, MCSE is not independent of the scale of the parameter, which means nterpreting whether MCSE is small enough requires domain expertise. For example, if we want to report the value of an estimated parameter to the second decimal, we need to be sure the MCSE is below the second decimal otherwise, we will be, wrongly, reporting a higher precision than we really have.

**KEY POINT 2**: We should check the MCSE only once we are sure ESS is high enough and $\hat{R}$ is low enough; otherwise, MCSE is of no use, because an ESS that is too high and/or an $\hat{R}$ value that is too low are sufficient to indicate a poor sampling method that would lead to high error rates.

# Diagnostic 4: Plots
Visual diagnosis of the sampling method:

- [Trace plots](https://bayesiancomputationbook.com/markdown/chp_02.html#trace-plots)
- [Autocorrelation plots](https://bayesiancomputationbook.com/markdown/chp_02.html#autocorrelation-plots)
- [Rank plots](https://bayesiancomputationbook.com/markdown/chp_02.html#rank-plots)

# Diagnostic 5: Divergences
To be continued...